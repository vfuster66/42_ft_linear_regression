# ğŸ“Š FT_LINEAR_REGRESSION - ImplÃ©mentation d'une rÃ©gression linÃ©aire simple

## ğŸ“Œ Objectif du projet
Ce projet implÃ©mente un modÃ¨le de **rÃ©gression linÃ©aire simple** afin de **prÃ©dire le prix d'un vÃ©hicule en fonction de son kilomÃ©trage**.  
L'objectif est d'utiliser **la descente de gradient** pour ajuster les paramÃ¨tres du modÃ¨le et obtenir **une prÃ©diction fiable**.

---

## ğŸ“– Qu'est-ce que la rÃ©gression linÃ©aire ?

### ğŸ”¹ DÃ©finition
La **rÃ©gression linÃ©aire** est une mÃ©thode statistique permettant **de modÃ©liser la relation entre une variable dÃ©pendante (Y) et une ou plusieurs variables indÃ©pendantes (X)** Ã  lâ€™aide dâ€™une **droite dâ€™Ã©quation** :

\[
Y = \theta_0 + \theta_1 X + \epsilon
\]

oÃ¹ :
- **\( Y \)** est la variable cible (ici, le prix dâ€™un vÃ©hicule),
- **\( X \)** est la variable explicative (ici, le kilomÃ©trage du vÃ©hicule),
- **\( \theta_0 \)** est lâ€™ordonnÃ©e Ã  lâ€™origine (le prix estimÃ© quand \( X = 0 \)),
- **\( \theta_1 \)** est la pente (la variation du prix en fonction du kilomÃ©trage),
- **\( \epsilon \)** reprÃ©sente lâ€™erreur de modÃ©lisation.

La rÃ©gression linÃ©aire trouve **les valeurs optimales de \( \theta_0 \) et \( \theta_1 \)** qui **minimisent lâ€™erreur** entre les prÃ©dictions et les valeurs rÃ©elles.

---

## ğŸ“Œ Ã€ quoi sert la rÃ©gression linÃ©aire ?

### ğŸ”¹ Exemples dâ€™utilisation
La rÃ©gression linÃ©aire est **utilisÃ©e dans de nombreux domaines** pour **prÃ©dire des valeurs futures** ou **analyser lâ€™impact dâ€™une variable sur une autre**.

ğŸ”¹ **Ã‰conomie et finance**  
- PrÃ©diction des **ventes** dâ€™un produit en fonction de la publicitÃ©.
- Estimation de **lâ€™Ã©volution des prix immobiliers** selon la surface.

ğŸ”¹ **Marketing & Business Intelligence**  
- PrÃ©diction du **chiffre dâ€™affaires** dâ€™une entreprise en fonction des tendances du marchÃ©.
- Analyse de lâ€™impact dâ€™une **campagne publicitaire** sur les ventes.

ğŸ”¹ **Science et environnement**  
- PrÃ©diction des **tempÃ©ratures futures** en fonction des annÃ©es.
- Estimation du **taux de pollution** en fonction du trafic routier.

ğŸ”¹ **Domaine mÃ©dical**  
- PrÃ©diction du **risque de maladies** en fonction du mode de vie (ex : poids, tabagisme).
- Estimation de lâ€™espÃ©rance de vie en fonction de critÃ¨res socio-Ã©conomiques.

### ğŸ”¹ Pourquoi utiliser la rÃ©gression linÃ©aire ?
âœ… **Facile Ã  comprendre et interprÃ©ter** ğŸ“Š  
âœ… **ModÃ¨le rapide et efficace** ğŸš€  
âœ… **Exige peu de donnÃ©es** ğŸ“‰  
âœ… **Applicable Ã  de nombreux domaines** ğŸŒ  

---

## ğŸ—ï¸ Structure du projet

```bash
. â”œâ”€â”€ app/ 
  â”‚ â”œâ”€â”€ train.py # Script d'entraÃ®nement du modÃ¨le 
  â”‚ â”œâ”€â”€ predict.py # Script de prÃ©diction du prix
  â”‚ â”œâ”€â”€ evaluate.py # Ã‰valuation de la prÃ©cision du modÃ¨le
  â”‚ â”œâ”€â”€ data.csv # Dataset d'entraÃ®nement
  â”‚ â”œâ”€â”€ model.json # ModÃ¨le sauvegardÃ© aprÃ¨s entraÃ®nement
  â”‚ â”œâ”€â”€ requirements.txt # DÃ©pendances Python
  â”‚ â”œâ”€â”€ tests/ # Dossier des tests unitaires
  â”‚ â”‚ â”œâ”€â”€ test_train.py # Tests pour train.py
  â”‚ â”‚ â”œâ”€â”€ test_predict.py # Tests pour predict.py
  â”‚ â”‚ â”œâ”€â”€ test_evaluate.py # Tests pour evaluate.py
  â”œâ”€â”€ Dockerfile # Image Docker du projet
  â”œâ”€â”€ Makefile # Commandes automatisÃ©es
  â”œâ”€â”€ README.md # Documentation du projet
```

---

## ğŸš€ Installation & ExÃ©cution

### **1ï¸âƒ£ PrÃ©requis**
- **Python 3.10+**
- **Docker & Docker Compose** installÃ© sur votre machine

### **2ï¸âƒ£ Cloner le projet**
```sh
git clone https://github.com/votre-repo/42_ft_linear_regression.git
cd 42_ft_linear_regression
```

### **3ï¸âƒ£ Construire l'image Docker**

```bash
make build
```

### **4ï¸âƒ£ EntraÃ®ner le modÃ¨le**

```bash
make train
```

ğŸ“Š EntraÃ®ne le modÃ¨le et sauvegarde les paramÃ¨tres Î¸0 et Î¸1.

### **5ï¸âƒ£ Faire une prÃ©diction**

```bash
make predict
```

ğŸš— Demande un kilomÃ©trage et retourne le prix estimÃ©.

### **6ï¸âƒ£ Ã‰valuer la prÃ©cision du modÃ¨le**

```bash
make evaluate
```

ğŸ“ Affiche l'erreur quadratique moyenne (MSE) pour mesurer la prÃ©cision.

### **7ï¸âƒ£ ExÃ©cuter les tests unitaires**

```bash
make test
```

âœ… VÃ©rifie que tous les scripts fonctionnent correctement.

---

## ğŸ“Š Explication du modÃ¨le

La rÃ©gression linÃ©aire modÃ©lise la relation entre **le kilomÃ©trage dâ€™un vÃ©hicule et son prix de revente** :

\[
\text{estimatePrice} = \theta_0 + \theta_1 \times \text{mileage}
\]

Le modÃ¨le ajuste **Î¸â‚€** et **Î¸â‚** grÃ¢ce Ã  **lâ€™algorithme de descente de gradient** :

\[
\theta_0 = \theta_0 - \alpha \times \frac{1}{m} \sum_{i=1}^{m} (\text{estimatePrice} - \text{price})
\]

\[
\theta_1 = \theta_1 - \alpha \times \frac{1}{m} \sum_{i=1}^{m} (\text{estimatePrice} - \text{price}) \times \text{mileage}
\]

ğŸ’¡ **Explication :**
- **`Î¸â‚€`** reprÃ©sente l'ordonnÃ©e Ã  l'origine (**le prix de base du vÃ©hicule**).
- **`Î¸â‚`** est la pente de la rÃ©gression (**lâ€™impact du kilomÃ©trage sur le prix**).
- **La descente de gradient** met Ã  jour `Î¸â‚€` et `Î¸â‚` Ã  chaque itÃ©ration pour minimiser l'erreur.

ğŸ”¢ **Objectif :** Trouver `Î¸â‚€` et `Î¸â‚` de maniÃ¨re Ã  rÃ©duire l'erreur entre les prix rÃ©els et les prix prÃ©dits.

---

## âœ… RÃ©sultats & Performance

### ğŸ“Œ ParamÃ¨tres entraÃ®nÃ©s :
```bash
Î¸0 = 8499.55
Î¸1 = -0.021448
```

### ğŸ“Œ Erreur quadratique moyenne (MSE) aprÃ¨s entraÃ®nement :
```bash
MSE = 445645.25
```

---

## ğŸš€ Pour aller plus loin

### ğŸ”¹ Limites de la rÃ©gression linÃ©aire
Bien que la **rÃ©gression linÃ©aire** soit un modÃ¨le simple et efficace, elle prÃ©sente **certaines limites** :
- **HypothÃ¨se de linÃ©aritÃ©** : elle suppose que la relation entre les variables est **toujours une droite**, ce qui **n'est pas toujours vrai**.
- **SensibilitÃ© aux valeurs aberrantes** : quelques points extrÃªmes peuvent **fortement influencer** le modÃ¨le.
- **Ne prend pas en compte les interactions complexes** entre plusieurs variables.

Pour pallier ces limites, **d'autres modÃ¨les existent** en Machine Learning.

---

## ğŸ“Œ Autres modÃ¨les de rÃ©gression

### 1ï¸âƒ£ **RÃ©gression polynomiale**  
ğŸ’¡ **Permet d'ajuster des courbes au lieu de droites**.  
Elle ajoute des **termes quadratiques, cubiques, etc.** pour mieux **capturer des relations non linÃ©aires**.

\[
Y = \theta_0 + \theta_1 X + \theta_2 X^2 + \theta_3 X^3 + ... + \epsilon
\]

âœ… **Avantage** : fonctionne mieux pour des relations courbes.  
âŒ **InconvÃ©nient** : risque d'**overfitting** si trop de termes sont ajoutÃ©s.

---

### 2ï¸âƒ£ **RÃ©gression logarithmique et exponentielle**  
ğŸ’¡ **Utile lorsque les donnÃ©es suivent une croissance ou dÃ©croissance exponentielle**.  
- **RÃ©gression exponentielle** : modÃ¨le la croissance rapide d'une valeur  
  \[
  Y = a \times e^{bX}
  \]
- **RÃ©gression logarithmique** : adaptÃ©e aux phÃ©nomÃ¨nes oÃ¹ la croissance ralentit avec le temps  
  \[
  Y = a + b \times \log(X)
  \]

âœ… **Avantage** : s'adapte aux tendances exponentielles.  
âŒ **InconvÃ©nient** : moins intuitif Ã  interprÃ©ter.

---

### 3ï¸âƒ£ **RÃ©gression multiple**  
ğŸ’¡ **Utilise plusieurs variables pour amÃ©liorer les prÃ©dictions**.  
Exemple : prÃ©dire le prix dâ€™un appartement en fonction de **sa surface, son quartier et son annÃ©e de construction**.

\[
Y = \theta_0 + \theta_1 X_1 + \theta_2 X_2 + \theta_3 X_3 + ... + \epsilon
\]

âœ… **Avantage** : prend en compte plusieurs facteurs.  
âŒ **InconvÃ©nient** : nÃ©cessite un dataset plus grand et un bon prÃ©traitement des donnÃ©es.

---

## ğŸ“Œ ModÃ¨les plus avancÃ©s (Machine Learning)

### 4ï¸âƒ£ **RÃ©gression Ridge & Lasso** (Regularization)  
ğŸ’¡ **Ajoutent une pÃ©nalitÃ© pour Ã©viter le sur-ajustement (overfitting).**  
- **RÃ©gression Ridge** : rÃ©duit l'importance des coefficients avec une pÃ©nalitÃ© **L2**.
- **RÃ©gression Lasso** : force certains coefficients Ã  **zÃ©ro** pour sÃ©lectionner automatiquement les variables les plus importantes.

âœ… **Avantage** : empÃªche les modÃ¨les trop complexes dâ€™overfitter.  
âŒ **InconvÃ©nient** : plus difficile Ã  interprÃ©ter qu'une simple rÃ©gression linÃ©aire.

---

### 5ï¸âƒ£ **Arbres de dÃ©cision et forÃªts alÃ©atoires** ğŸŒ³  
ğŸ’¡ **Utilisent des divisions successives pour prÃ©dire une valeur**.  
Ils fonctionnent bien quand la relation entre les variables est **trÃ¨s non linÃ©aire**.

âœ… **Avantage** : fonctionne sur des relations complexes sans besoin de transformation des donnÃ©es.  
âŒ **InconvÃ©nient** : peut Ãªtre instable si lâ€™on ne rÃ¨gle pas bien les hyperparamÃ¨tres.

---

### 6ï¸âƒ£ **RÃ©seaux de neurones (Deep Learning) ğŸ§ **  
ğŸ’¡ **UtilisÃ©s pour des prÃ©dictions ultra prÃ©cises** grÃ¢ce Ã  plusieurs couches de neurones artificiels.

âœ… **Avantage** : excellent pour les **grands volumes de donnÃ©es** et les **relations trÃ¨s complexes**.  
âŒ **InconvÃ©nient** : demande **beaucoup de donnÃ©es et de puissance de calcul**.

---

## ğŸ“š Sources & Documentation

- ğŸ“– [Cours OpenClassrooms - RÃ©gression linÃ©aire](https://openclassrooms.com/fr/courses/4525326)
- ğŸ“– [Descente de gradient - WikipÃ©dia](https://fr.wikipedia.org/wiki/Descente_de_gradient)
- ğŸ“– [Documentation officielle `numpy`](https://numpy.org/doc/)
- ğŸ“– [Documentation officielle `pytest`](https://docs.pytest.org/en/stable/)
- ğŸ“– [Cours OpenClassrooms - Machine Learning](https://openclassrooms.com/fr/courses/4525326)
- ğŸ“– [Comparaison des modÃ¨les de rÃ©gression](https://scikit-learn.org/stable/supervised_learning.html)
- ğŸ“– [Documentation officielle `scikit-learn`](https://scikit-learn.org/stable/)
- ğŸ“– [Tutoriel sur les arbres de dÃ©cision](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)

